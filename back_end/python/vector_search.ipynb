{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sakib56/TTDS-G35-CW3/blob/main/back_end/python/vector_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJLQoimyVyQ8"
      },
      "source": [
        "### Uncomment and run the following cells if you work on GCP. Change runtime type to GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install transformers>=3.3.1 sentence-transformers>=0.3.8 pandas>=1.1.2 faiss-cpu>=1.6.1 numpy>=1.19.2 folium>=0.2.1 streamlit>=0.62.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaOE4h_25XNZ"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "This is mounting my (Kenza) drive to the collab notebook. I stored the wikidata there.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJ0elPldgeKZ",
        "outputId": "ed513da6-b8ce-489c-cc4c-7e9c1c4edaef"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbnscDwgVyRW"
      },
      "source": [
        "### Before we begin, make sure you restart (not factory reset) the runtime so that the relevant packages are used"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "v7ftrzzmVyRX"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fU2i4vlCVyRc"
      },
      "outputs": [],
      "source": [
        "# %autoreload 2\n",
        "\n",
        "# Used to create the dense document vectors.\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import sys\n",
        "\n",
        "# Used to create and store the Faiss index.\n",
        "import faiss\n",
        "import numpy as np\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "\n",
        "import xml\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "import re\n",
        "ps = PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dM0fIwUV6M7c"
      },
      "outputs": [],
      "source": [
        "def vector_search(query, model, index, num_results=10):\n",
        "    \"\"\"Tranforms query to vector using a pretrained, sentence-level \n",
        "    DistilBERT model and finds similar vectors using FAISS.\n",
        "    Args:\n",
        "        query (str): User query that should be more than a sentence long.\n",
        "        model (sentence_transformers.SentenceTransformer.SentenceTransformer)\n",
        "        index (`numpy.ndarray`): FAISS index that needs to be deserialized.\n",
        "        num_results (int): Number of results to return.\n",
        "    Returns:\n",
        "        D (:obj:`numpy.array` of `float`): Distance between results and query.\n",
        "        I (:obj:`numpy.array` of `int`): Paper ID of the results.\n",
        "    \n",
        "    \"\"\"\n",
        "    # query = ps.stem(query)\n",
        "    vector = model.encode(list(query))\n",
        "    D, I = index.search(np.array(vector).astype(\"float32\"), k=num_results)\n",
        "    return D, I\n",
        "\n",
        "\n",
        "def id2details(I):\n",
        "    \"\"\"Returns the paper titles based on the paper index.\"\"\"\n",
        "    return [worker.pids[str(idx)] for idx in I[0]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyRG1wZLVyRw"
      },
      "source": [
        "The [Sentence Transformers library](https://github.com/UKPLab/sentence-transformers) offers pretrained transformers that produce SOTA sentence embeddings. Checkout this [spreadsheet](https://docs.google.com/spreadsheets/d/14QplCdTCDwEmTqrn1LH4yrbKvdogK4oQvYO1K1aPR5M/) with all the available models.\n",
        "\n",
        "In this tutorial, we will use the `distilbert-base-nli-stsb-mean-tokens` model which has the best performance on Semantic Textual Similarity tasks among the DistilBERT versions. Moreover, although it's slightly worse than BERT, it is quite faster thanks to having a smaller size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjF6CrwUVyRx",
        "outputId": "a7fd5d08-eefa-4ef2-c204-8ff22e7eeed7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "# Instantiate the sentence-level DistilBERT\n",
        "model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
        "# Check if GPU is available and use it\n",
        "if torch.cuda.is_available():\n",
        "    model = model.to(torch.device(\"cuda\"))\n",
        "print(model.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-OTxFb--YYD9"
      },
      "outputs": [],
      "source": [
        "class wikiHandler(xml.sax.ContentHandler):\n",
        "\n",
        "    def __init__(self, searchClass):\n",
        "        self.tag = \"\"\n",
        "        self.pid = \"\"\n",
        "        self.title = \"\"\n",
        "        self.text = \"\"\n",
        "        self.searcher = searchClass\n",
        "\n",
        "    def ended(self):\n",
        "        self.executor.shutdown()\n",
        "\n",
        "    def startElement(self, tag, argument):\n",
        "        self.tag = tag\n",
        "\n",
        "    def characters(self, content):\n",
        "        if self.tag == \"id\" and not content.isspace() and (self.pid == \"\" or len(self.pid) == 0):\n",
        "            self.pid = content\n",
        "        if self.tag == \"title\":\n",
        "            self.title += content\n",
        "        if self.tag == \"text\":\n",
        "            self.text += content\n",
        "\n",
        "    def endElement(self, tag):\n",
        "        if tag == \"page\":\n",
        "            self.searcher.perpage({\"pid\":self.pid, \"title\":self.title, \"text\":self.text})\n",
        "            self.pid = \"\"\n",
        "            self.title = \"\"\n",
        "            self.text = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Y_GS0_CWVyR1",
        "outputId": "b8395855-fcf8-46b8-b493-991ca09b5b43"
      },
      "outputs": [],
      "source": [
        "class encoder():\n",
        "    def __init__(self):\n",
        "        self.embeddings = []\n",
        "        self.partials = []\n",
        "        self.partial_ids={}\n",
        "        self.pids = {}\n",
        "        self.quantizer = faiss.IndexFlatL2(768)\n",
        "        self.nlist = 256\n",
        "        self.index = faiss.IndexIVFFlat(self.quantizer, 768, self.nlist)\n",
        "        self.count = 0\n",
        "\n",
        "    def perpage(self, text):\n",
        "        train_timer = 9984\n",
        "        f = f\"./faiss_index{self.nlist}_{train_timer}_.pickle\"\n",
        "\n",
        "        try:\n",
        "            self.pids[text[\"pid\"]] = text[\"title\"]\n",
        "            self.partial_ids[text[\"pid\"]] = text[\"title\"]\n",
        "            encoding = model.encode(text[\"text\"])\n",
        "            self.embeddings.append(encoding)\n",
        "            self.partials.append(encoding)\n",
        "            self.count += 1\n",
        "            if self.count % train_timer == 0:\n",
        "                self.partials = np.array([embedding for embedding in self.partials]).astype(\"float32\")\n",
        "                self.index.train(self.partials)\n",
        "                index = faiss.IndexIDMap(self.index)\n",
        "                index.add_with_ids(self.partials, np.array(list(self.partial_ids.keys())).astype('int64'))\n",
        "                with open(f, \"ab+\") as h:\n",
        "                    pickle.dump(faiss.serialize_index(index), h)\n",
        "                self.partials = []\n",
        "                self.partial_ids = {}\n",
        "                index.reset()\n",
        "        except KeyboardInterrupt:\n",
        "            sys.exit()\n",
        "        except Exception as e:\n",
        "            self.partials = []\n",
        "            self.partial_ids = {}\n",
        "            if 'index' in locals():\n",
        "                if index is not None:\n",
        "                    index.reset()\n",
        "            print(self.count, e)\n",
        "            pass\n",
        "        print(self.count, end=\"\\r\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "worker = encoder()\n",
        "parser = xml.sax.make_parser()  \n",
        "parser.setFeature(xml.sax.handler.feature_namespaces, 0)\n",
        "handler = wikiHandler(worker)\n",
        "parser.setContentHandler(handler)\n",
        "\n",
        "#### INPUT PATH TO LONG XML!!!!!\n",
        "parser.parse(\"./enwiki-20220301-pages-articles-multistream.xml\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYCBk6prpRjk"
      },
      "outputs": [],
      "source": [
        "# Should the entire thing run properly, try running this to make sure index is 100% correct:\n",
        "quantizer = faiss.IndexFlatL2(768)\n",
        "nlist = 256\n",
        "index = faiss.IndexIVFFlat(quantizer, 768, nlist)\n",
        "worker.embeddings = np.array([embedding for embedding in worker.embeddings]).astype(\"float32\")\n",
        "index.train(worker.embeddings)\n",
        "index = faiss.IndexIDMap(index)\n",
        "# index.add_with_ids(worker.embeddings, np.array(list(worker.pids.keys())).astype('int64'))\n",
        "index.add_with_ids(\n",
        "  worker.embeddings, \n",
        "  np.array(list(worker.pids.keys())).astype('int64')[:worker.embeddings.shape[0]].astype('int64'))\n",
        "with open(\"./final_faiss_index2.pickle\", \"ab+\") as h:\n",
        "  pickle.dump(faiss.serialize_index(index), h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gE7w-RJbVyR6",
        "outputId": "a69a12aa-000b-4f3a-c31f-e03b30476e4a"
      },
      "outputs": [],
      "source": [
        "print(f'Number of articles processed: {len(worker.embeddings)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFKvRb4QY-DL"
      },
      "source": [
        "\n",
        "## Putting all together\n",
        "\n",
        "So far, we've built a Faiss index using the wikidata text vectors we encoded with a sentence-DistilBERT model. That's helpful but in a real case scenario, we would have to work with unseen data. To query the index with an unseen query and retrieve its most relevant documents, we would have to do the following:\n",
        "\n",
        "1. Encode the stemmed query with the same sentence-DistilBERT model we used for the rest of the abstract vectors.\n",
        "2. Change its data type to float32.\n",
        "3. Search the index with the encoded query.\n",
        "\n",
        "IDEA: Use the Answer of the Question Answering option as the input query for vector search or let the user write a query for vector search or both.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDhftkrhX99T"
      },
      "outputs": [],
      "source": [
        "user_query = \"\"\"Artificial Intelligence\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AFhbGnWZpWN",
        "outputId": "ddf8984d-4c8b-4ccf-d790-539289cc80fc"
      },
      "outputs": [],
      "source": [
        "# For convenience, I've wrapped all steps in the vector_search function.\n",
        "# It takes four arguments: \n",
        "# A query, the sentence-level transformer, the Faiss index and the number of requested results\n",
        "D, I = vector_search([user_query], model, index, num_results=10)\n",
        "print(f'L2 distance: {D.flatten().tolist()}\\n\\nMAG paper IDs: {I.flatten().tolist()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbanjBhBZtWZ",
        "outputId": "bf4fa949-2179-4533-da7e-f21dd58c89ce"
      },
      "outputs": [],
      "source": [
        "# Fetching the paper titles based on their index\n",
        "id2details(I)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "chosen_pickle=faiss_index5690k.pickle\n"
          ]
        }
      ],
      "source": [
        "# Run Search from Pickle Index\n",
        "import os\n",
        "fpath = \"D:/TTDS-G35-CW3-FINAL/back_end/python/vctr_idx/\"\n",
        "pickles = [f for f in os.listdir(fpath) if \"pickle\" in f]\n",
        "\n",
        "chosen_pickle = pickles[1]\n",
        "print(f\"chosen_pickle={chosen_pickle}\")\n",
        "with open(fpath+chosen_pickle,'rb') as infile:\n",
        "        index = pickle.load(infile)\n",
        "index = faiss.deserialize_index(index)\n",
        "user_query = \"\"\"Artificial Intelligence\"\"\"\n",
        "D, I = vector_search([user_query], model, index, num_results=10)\n",
        "print(f'L2 distance: {D.flatten().tolist()}\\n\\nMAG paper IDs: {I.flatten().tolist()}')\n",
        "# worker = encoder()\n",
        "# id2details(I)\n",
        "print([f\"https://en.wikipedia.org/?curid={id}\" for id in I.flatten().tolist()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(247429.5652173913, 212608.68343259502)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "x = [\n",
        "0,\n",
        "838656,\n",
        "1038336,\n",
        "1377792,\n",
        "1467648,\n",
        "1687296,\n",
        "1847040,\n",
        "1946880,\n",
        "2046720,\n",
        "2426112,\n",
        "2615808,\n",
        "2735616,\n",
        "3274752,\n",
        "3704064,\n",
        "3763968,\n",
        "3903744,\n",
        "4113408,\n",
        "4203264,\n",
        "4233216,\n",
        "4462848,\n",
        "5181696,\n",
        "5241600,\n",
        "5650944,\n",
        "5690880,\n",
        "]\n",
        "\n",
        "y = [b-a for a,b in zip(x, x[1:])]\n",
        "y = np.array(y)\n",
        "y.mean(), y.var()**0.5\n",
        "# x."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "machine_shape": "hm",
      "name": "001-vector-search.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "e71b966450652ff8f98dd4b08bbe58c39e8c3b773236885388bb8e4deeef0722"
    },
    "kernelspec": {
      "display_name": "Python [conda env:myenv]",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
