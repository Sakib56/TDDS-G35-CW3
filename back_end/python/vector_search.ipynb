{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "001-vector-search.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python [conda env:myenv]",
      "language": "python",
      "name": "conda-env-myenv-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJLQoimyVyQ8"
      },
      "source": [
        "### Uncomment and run the following cells if you work on GCP. Change runtime type to GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sOhWL6UVyRQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "910729ce-298a-43ad-9118-8e5b5f0d8142"
      },
      "source": [
        "pip install torch==1.8.1 transformers==3.3.1 sentence-transformers==0.3.8 pandas==1.1.2 faiss-cpu==1.6.1 numpy==1.19.2 folium==0.2.1 streamlit==0.62.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==1.8.1\n",
            "  Downloading torch-1.8.1-cp37-cp37m-manylinux1_x86_64.whl (804.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 804.1 MB 2.6 kB/s \n",
            "\u001b[?25hCollecting transformers==3.3.1\n",
            "  Downloading transformers-3.3.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 65.5 MB/s \n",
            "\u001b[?25hCollecting sentence-transformers==0.3.8\n",
            "  Downloading sentence-transformers-0.3.8.tar.gz (66 kB)\n",
            "\u001b[K     |████████████████████████████████| 66 kB 6.7 MB/s \n",
            "\u001b[?25hCollecting pandas==1.1.2\n",
            "  Downloading pandas-1.1.2-cp37-cp37m-manylinux1_x86_64.whl (10.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.5 MB 25.7 MB/s \n",
            "\u001b[?25hCollecting faiss-cpu==1.6.1\n",
            "  Downloading faiss_cpu-1.6.1-cp37-cp37m-manylinux2010_x86_64.whl (7.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 20.9 MB/s \n",
            "\u001b[?25hCollecting numpy==1.19.2\n",
            "  Downloading numpy-1.19.2-cp37-cp37m-manylinux2010_x86_64.whl (14.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.5 MB 25.5 MB/s \n",
            "\u001b[?25hCollecting folium==0.2.1\n",
            "  Downloading folium-0.2.1.tar.gz (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 10.7 MB/s \n",
            "\u001b[?25hCollecting streamlit==0.62.0\n",
            "  Downloading streamlit-0.62.0-py2.py3-none-any.whl (7.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 27.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1) (3.10.0.2)\n",
            "Collecting tokenizers==0.8.1.rc2\n",
            "  Downloading tokenizers-0.8.1rc2-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 46.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1) (21.3)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 69.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1) (4.63.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1) (2.23.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 68.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1) (3.6.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.3.8) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.3.8) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.3.8) (3.2.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.2) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.2) (2.8.2)\n",
            "Requirement already satisfied: Jinja2 in /usr/local/lib/python3.7/dist-packages (from folium==0.2.1) (2.11.3)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from streamlit==0.62.0) (0.8.1)\n",
            "Collecting pydeck>=0.1.dev5\n",
            "  Downloading pydeck-0.7.1-py2.py3-none-any.whl (4.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 57.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==0.62.0) (4.2.4)\n",
            "Collecting validators\n",
            "  Downloading validators-0.18.2-py3-none-any.whl (19 kB)\n",
            "Collecting blinker\n",
            "  Downloading blinker-1.4.tar.gz (111 kB)\n",
            "\u001b[K     |████████████████████████████████| 111 kB 93.2 MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "  Downloading boto3-1.21.18-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 72.4 MB/s \n",
            "\u001b[?25hCollecting enum-compat\n",
            "  Downloading enum_compat-0.0.3-py3-none-any.whl (1.3 kB)\n",
            "Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==0.62.0) (5.1.1)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from streamlit==0.62.0) (1.5.1)\n",
            "Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==0.62.0) (4.2.0)\n",
            "Collecting watchdog\n",
            "  Downloading watchdog-2.1.6-py3-none-manylinux2014_x86_64.whl (76 kB)\n",
            "\u001b[K     |████████████████████████████████| 76 kB 7.0 MB/s \n",
            "\u001b[?25hCollecting toml\n",
            "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
            "Collecting base58\n",
            "  Downloading base58-2.1.1-py3-none-any.whl (5.6 kB)\n",
            "Collecting botocore>=1.13.44\n",
            "  Downloading botocore-1.24.18-py3-none-any.whl (8.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.6 MB 73.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==0.62.0) (7.1.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==0.62.0) (7.1.2)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==0.62.0) (3.17.3)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit==0.62.0) (4.3.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit==0.62.0) (0.11.2)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit==0.62.0) (0.4)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.8-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 90.8 MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit==0.62.0) (0.18.1)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit==0.62.0) (5.4.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit==0.62.0) (21.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit==0.62.0) (4.11.2)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=3.0->altair>=3.2.0->streamlit==0.62.0) (3.7.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.0->streamlit==0.62.0) (1.15.0)\n",
            "Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit==0.62.0) (7.6.5)\n",
            "Collecting ipykernel>=5.1.2\n",
            "  Downloading ipykernel-6.9.2-py3-none-any.whl (130 kB)\n",
            "\u001b[K     |████████████████████████████████| 130 kB 76.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit==0.62.0) (5.1.1)\n",
            "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.62.0) (0.1.3)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.62.0) (1.5.4)\n",
            "Collecting ipython>=7.23.1\n",
            "  Downloading ipython-7.32.0-py3-none-any.whl (793 kB)\n",
            "\u001b[K     |████████████████████████████████| 793 kB 63.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jupyter-client<8.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.62.0) (5.3.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.62.0) (5.4.8)\n",
            "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.62.0) (1.0.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.62.0) (0.2.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.62.0) (57.4.0)\n",
            "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
            "  Downloading prompt_toolkit-3.0.28-py3-none-any.whl (380 kB)\n",
            "\u001b[K     |████████████████████████████████| 380 kB 40.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.62.0) (2.6.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.62.0) (0.18.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.62.0) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.62.0) (0.7.5)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.62.0) (4.8.0)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.62.0) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.62.0) (3.5.2)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.62.0) (5.1.3)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.62.0) (1.0.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.62.0) (0.8.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2->folium==0.2.1) (2.0.1)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.62.0) (4.9.2)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.62.0) (22.3.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.62.0) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.62.0) (0.2.5)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.62.0) (5.3.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.62.0) (0.13.3)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.62.0) (5.6.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.62.0) (1.8.0)\n",
            "Collecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.2-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 10.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.62.0) (0.8.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.62.0) (1.5.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.62.0) (0.7.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.62.0) (4.1.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.62.0) (0.6.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.62.0) (0.5.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.3.1) (3.0.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.1) (2.10)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 87.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.1) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.1) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.3.1) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers==0.3.8) (3.1.0)\n",
            "Building wheels for collected packages: sentence-transformers, folium, blinker\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-0.3.8-py3-none-any.whl size=101994 sha256=fce557644ef4be9021005d044ead9ee3c90fc94929492c5afb38bcbe80f01161\n",
            "  Stored in directory: /root/.cache/pip/wheels/1c/43/65/fe0f3ea9327623e749a79eb5dfad85a809c84064b1cc4682c1\n",
            "  Building wheel for folium (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for folium: filename=folium-0.2.1-py3-none-any.whl size=79808 sha256=1d975090b765955a07fdca38b830e0184a69586e9122d1e79b94dfbfade3cb1b\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/f0/3a/3f79a6914ff5affaf50cabad60c9f4d565283283c97f0bdccf\n",
            "  Building wheel for blinker (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for blinker: filename=blinker-1.4-py3-none-any.whl size=13478 sha256=d96494e631de4de8614161575483c591c6e5e532abcfc762ba8b4192b535a502\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/f5/18/df711b66eb25b21325c132757d4314db9ac5e8dabeaf196eab\n",
            "Successfully built sentence-transformers folium blinker\n",
            "Installing collected packages: prompt-toolkit, ipython, ipykernel, urllib3, jmespath, numpy, botocore, tokenizers, sentencepiece, sacremoses, s3transfer, pandas, watchdog, validators, transformers, torch, toml, pydeck, enum-compat, boto3, blinker, base58, streamlit, sentence-transformers, folium, faiss-cpu\n",
            "  Attempting uninstall: prompt-toolkit\n",
            "    Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "  Attempting uninstall: ipykernel\n",
            "    Found existing installation: ipykernel 4.10.1\n",
            "    Uninstalling ipykernel-4.10.1:\n",
            "      Successfully uninstalled ipykernel-4.10.1\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.5\n",
            "    Uninstalling numpy-1.21.5:\n",
            "      Successfully uninstalled numpy-1.21.5\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.3.5\n",
            "    Uninstalling pandas-1.3.5:\n",
            "      Successfully uninstalled pandas-1.3.5\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "  Attempting uninstall: folium\n",
            "    Found existing installation: folium 0.8.3\n",
            "    Uninstalling folium-0.8.3:\n",
            "      Successfully uninstalled folium-0.8.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.8.1 which is incompatible.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.8.1 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.8.1 which is incompatible.\n",
            "tensorflow 2.8.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.28 which is incompatible.\n",
            "google-colab 1.0.0 requires ipykernel~=4.10, but you have ipykernel 6.9.2 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.32.0 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed base58-2.1.1 blinker-1.4 boto3-1.21.18 botocore-1.24.18 enum-compat-0.0.3 faiss-cpu-1.6.1 folium-0.2.1 ipykernel-6.9.2 ipython-7.32.0 jmespath-0.10.0 numpy-1.19.2 pandas-1.1.2 prompt-toolkit-3.0.28 pydeck-0.7.1 s3transfer-0.5.2 sacremoses-0.0.47 sentence-transformers-0.3.8 sentencepiece-0.1.96 streamlit-0.62.0 tokenizers-0.8.1rc2 toml-0.10.2 torch-1.8.1 transformers-3.3.1 urllib3-1.25.11 validators-0.18.2 watchdog-2.1.6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "ipykernel",
                  "numpy",
                  "prompt_toolkit"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "This is mounting my (Kenza) drive to the collab notebook. I stored the wikidata there.\n"
      ],
      "metadata": {
        "id": "KaOE4h_25XNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJ0elPldgeKZ",
        "outputId": "003649fb-aaab-4b86-d3ed-e10959334cb9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bzip2 -d /content/drive/MyDrive/TTDS/enwiki-latest-pages-articles.xml.bz2"
      ],
      "metadata": {
        "id": "D2V5sW_TggQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbnscDwgVyRW"
      },
      "source": [
        "### Before we begin, make sure you restart (not factory reset) the runtime so that the relevant packages are used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7ftrzzmVyRX"
      },
      "source": [
        "%load_ext autoreload"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fU2i4vlCVyRc"
      },
      "source": [
        "%autoreload 2\n",
        "\n",
        "# Used to create the dense document vectors.\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Used to create and store the Faiss index.\n",
        "import faiss\n",
        "import numpy as np\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "\n",
        "import xml\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "import re\n",
        "ps = PorterStemmer()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vector_search(query, model, index, num_results=10):\n",
        "    \"\"\"Tranforms query to vector using a pretrained, sentence-level \n",
        "    DistilBERT model and finds similar vectors using FAISS.\n",
        "    Args:\n",
        "        query (str): User query that should be more than a sentence long.\n",
        "        model (sentence_transformers.SentenceTransformer.SentenceTransformer)\n",
        "        index (`numpy.ndarray`): FAISS index that needs to be deserialized.\n",
        "        num_results (int): Number of results to return.\n",
        "    Returns:\n",
        "        D (:obj:`numpy.array` of `float`): Distance between results and query.\n",
        "        I (:obj:`numpy.array` of `int`): Paper ID of the results.\n",
        "    \n",
        "    \"\"\"\n",
        "    # query = ps.stem(query)\n",
        "    vector = model.encode(list(query))\n",
        "    D, I = index.search(np.array(vector).astype(\"float32\"), k=num_results)\n",
        "    return D, I\n",
        "\n",
        "\n",
        "def id2details(I):\n",
        "    \"\"\"Returns the paper titles based on the paper index.\"\"\"\n",
        "    return [worker.pids[str(idx)] for idx in I[0]]"
      ],
      "metadata": {
        "id": "dM0fIwUV6M7c"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyRG1wZLVyRw"
      },
      "source": [
        "The [Sentence Transformers library](https://github.com/UKPLab/sentence-transformers) offers pretrained transformers that produce SOTA sentence embeddings. Checkout this [spreadsheet](https://docs.google.com/spreadsheets/d/14QplCdTCDwEmTqrn1LH4yrbKvdogK4oQvYO1K1aPR5M/) with all the available models.\n",
        "\n",
        "In this tutorial, we will use the `distilbert-base-nli-stsb-mean-tokens` model which has the best performance on Semantic Textual Similarity tasks among the DistilBERT versions. Moreover, although it's slightly worse than BERT, it is quite faster thanks to having a smaller size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjF6CrwUVyRx",
        "outputId": "1f378652-5ce2-40c6-f647-68872a93d240"
      },
      "source": [
        "# Instantiate the sentence-level DistilBERT\n",
        "model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
        "# Check if GPU is available and use it\n",
        "if torch.cuda.is_available():\n",
        "    model = model.to(torch.device(\"cuda\"))\n",
        "print(model.device)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 245M/245M [00:37<00:00, 6.53MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class wikiHandler(xml.sax.ContentHandler):\n",
        "\n",
        "    def __init__(self, searchClass):\n",
        "        self.tag = \"\"\n",
        "        self.pid = None\n",
        "        self.title = \"\"\n",
        "        self.text = \"\"\n",
        "        self.searcher = searchClass\n",
        "        #self.executor = ProcessPoolExecutor(max_workers=1)\n",
        "        self.progress = tqdm(total=70000000)\n",
        "\n",
        "    def ended(self):\n",
        "        self.progress.close()\n",
        "        self.executor.shutdown()\n",
        "\n",
        "    def startElement(self, tag, argument):\n",
        "        self.tag = tag\n",
        "\n",
        "    def characters(self, content):\n",
        "        if self.tag == \"id\" and not content.isspace() and self.pid == None:\n",
        "            self.pid = content\n",
        "        if self.tag == \"title\":\n",
        "            self.title += content\n",
        "        if self.tag == \"text\":\n",
        "            self.text += content\n",
        "\n",
        "    def endElement(self, tag):\n",
        "        self.progress.update(1)\n",
        "        if tag == \"page\":\n",
        "            self.searcher.perpage({\"pid\":self.pid, \"title\":self.title, \"text\":self.text})\n",
        "            self.pid = None\n",
        "            self.title = \"\"\n",
        "            self.text = \"\""
      ],
      "metadata": {
        "id": "-OTxFb--YYD9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_GS0_CWVyR1",
        "outputId": "3068ef8b-5e51-467b-b41d-0a058545ecb9"
      },
      "source": [
        "# Convert abstracts to vectors\n",
        "class encoder():\n",
        "\n",
        "  def __init__(self):\n",
        "    self.embeddings = []\n",
        "    self.pids = {}\n",
        "\n",
        "  def perpage(self, text):\n",
        "    self.pids[text[\"pid\"]] = text[\"title\"]\n",
        "    self.embeddings.append(model.encode(text[\"text\"]))\n",
        "\n",
        "worker = encoder()\n",
        "parser = xml.sax.make_parser()  \n",
        "parser.setFeature(xml.sax.handler.feature_namespaces, 0)\n",
        "handler = wikiHandler(worker)\n",
        "parser.setContentHandler(handler)\n",
        "\n",
        "parser.parse(\"/content/drive/MyDrive/TTDS/wikidata_notsoshort.xml\")\n",
        "\n",
        "# embeddings = model.encode(df.text.to_list(), show_progress_bar=True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 68740/70000000 [37:40<1986:14:21,  9.78it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gE7w-RJbVyR6",
        "outputId": "a69a12aa-000b-4f3a-c31f-e03b30476e4a"
      },
      "source": [
        "print(f'Shape of the vectorised abstract: {len(worker.embeddings)}')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the vectorised abstract: 4171\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGV4Je1EVyR_"
      },
      "source": [
        "## Vector similarity search with Faiss\n",
        "[Faiss](https://github.com/facebookresearch/faiss) is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, even ones that do not fit in RAM. \n",
        "    \n",
        "Faiss is built around the `Index` object which contains, and sometimes preprocesses, the searchable vectors. Faiss has a large collection of [indexes](https://github.com/facebookresearch/faiss/wiki/Faiss-indexes). You can even create [composite indexes](https://github.com/facebookresearch/faiss/wiki/Faiss-indexes-(composite)). Faiss handles collections of vectors of a fixed dimensionality d, typically a few 10s to 100s.\n",
        "\n",
        "**Note**: Faiss uses only 32-bit floating point matrices. This means that you will have to change the data type of the input before building the index.\n",
        "\n",
        "To learn more about Faiss, you can read their paper on [arXiv](https://arxiv.org/abs/1702.08734).\n",
        "\n",
        "To speed up the search, it is possible to segment the dataset into pieces. We define Voronoi cells in the d-dimensional space, and each database vector falls in one of the cells. At search time, only the database vectors y contained in the cell the query x falls in and a few neighboring ones are compared against the query vector.\n",
        "\n",
        "This is done via the IndexIVFFlat index. This type of index requires a training stage, that can be performed on any collection of vectors that has the same distribution as the database vectors. In this case we just use the database vectors themselves.\n",
        "\n",
        "The IndexIVFFlat also requires another index, the quantizer, that assigns vectors to Voronoi cells. Each cell is defined by a centroid, and finding the Voronoi cell a vector falls in consists in finding the nearest neighbor of the vector in the set of centroids. This is the task of the other index, which is typically an IndexFlatL2.\n",
        "\n",
        "There are two parameters to the search method: nlist, the number of cells, and nprobe, the number of cells (out of nlist) that are visited to perform a search. The search time roughly increases linearly with the number of probes plus some constant due to the quantization.\n",
        "\n",
        "To create an index with the `wikidata` abstract vectors, we will:\n",
        "1. Change the data type of the text vectors to float32.\n",
        "2. Build an index and pass it the dimension of the vectors it will operate on.\n",
        "3. Pass the index to IndexIDMap, an object that enables us to provide a custom list of IDs for the indexed vectors.\n",
        "4. Add the abstract vectors and their ID mapping to the index. In our case, we will map vectors to their page IDs from MAG."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kkUDtwHVyR_",
        "outputId": "3b879350-efb0-42d7-9ac1-70d14eded645"
      },
      "source": [
        "# Step 1: Change data type\n",
        "worker.embeddings = np.array([embedding for embedding in worker.embeddings]).astype(\"float32\")\n",
        "\n",
        "# Step 2: Instantiate the index\n",
        "quantizer = faiss.IndexFlatL2(worker.embeddings.shape[1])\n",
        "nlist = 100\n",
        "index = faiss.IndexIVFFlat(quantizer, worker.embeddings.shape[1], nlist)\n",
        "index.train(worker.embeddings)\n",
        "index = faiss.IndexIDMap(index)\n",
        "index.add_with_ids(worker.embeddings, np.array(list(worker.pids.keys())).astype('int64'))\n",
        "\n",
        "\n",
        "# # Step 3: Pass the index to IndexIDMap\n",
        "# index = faiss.IndexIDMap(index)\n",
        "\n",
        "# # Step 4: Add vectors and their IDs\n",
        "# index.add_with_ids(embeddings, np.array(df.id.values))\n",
        "\n",
        "print(f\"Number of vectors in the Faiss index: {index.ntotal}\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of vectors in the Faiss index: 4171\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yt1z-433VySE"
      },
      "source": [
        "### Searching the index\n",
        "The index we built will perform a k-nearest-neighbour search. We have to provide the number of neighbours to be returned. \n",
        "\n",
        "Let's query the index with an abstract from our dataset and retrieve the 10 most relevant documents. **The first one must be our query!**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "eEeJt7lYVySN",
        "outputId": "e358465a-f6de-4841-966e-ba9cfd82b723"
      },
      "source": [
        "# Wikidata Text\n",
        "df.iloc[300, 2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'redirect lycom t55 redirect categori shell R from incorrect name R from move'"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSuRcH85VySQ",
        "outputId": "d92b05ee-efb1-4e63-b678-1fe3e88cc03f"
      },
      "source": [
        "nprobe = 2  # find 2 most similar clusters \n",
        "k = 10  # return 10 nearest neighbours\n",
        "D, I = index.search(np.array([embeddings[300]]), k=10)\n",
        "print(f'L2 distance: {D.flatten().tolist()}\\n\\nWiki IDs: {I.flatten().tolist()}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L2 distance: [0.0, 103.7332763671875, 106.08882141113281, 108.4593734741211, 108.8836669921875, 118.27703094482422, 118.73644256591797, 123.32868957519531, 126.17843627929688, 126.55046081542969]\n",
            "\n",
            "Wiki IDs: [69171661, 69171548, 69171417, 69171555, 69171578, 69171644, 69171363, 69171299, 69171558, 69171463]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiO1pa4oVySU",
        "outputId": "17f893d8-f6a6-43a4-be02-796874a9d5b7"
      },
      "source": [
        "# Fetch the Wikipedia titles based on their index\n",
        "id2details(df, I, 'title')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Honeywell T55 Turboshaft Engine'],\n",
              " ['Mediterranean (Battle honour)'],\n",
              " ['Draft:Penguinxs/sandbox/List of Feminist Periodicals in the United States'],\n",
              " ['Masdar Institute of Science and Technology'],\n",
              " ['Draft:Vasudha Dhagamwar'],\n",
              " ['Draft:Jeevan Bahadur Shahi'],\n",
              " ['List of accidents and incidents involving the DC-3 in 1975'],\n",
              " ['List of accidents and incidents involving the DC-3 in the 1970s'],\n",
              " ['Mediterranean 1901–02 (Battle honour)'],\n",
              " ['Henri Louit']]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFKvRb4QY-DL"
      },
      "source": [
        "\n",
        "## Putting all together\n",
        "\n",
        "So far, we've built a Faiss index using the wikidata text vectors we encoded with a sentence-DistilBERT model. That's helpful but in a real case scenario, we would have to work with unseen data. To query the index with an unseen query and retrieve its most relevant documents, we would have to do the following:\n",
        "\n",
        "1. Encode the stemmed query with the same sentence-DistilBERT model we used for the rest of the abstract vectors.\n",
        "2. Change its data type to float32.\n",
        "3. Search the index with the encoded query.\n",
        "\n",
        "IDEA: Use the Answer of the Question Answering option as the input query for vector search or let the user write a query for vector search or both.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDhftkrhX99T"
      },
      "source": [
        "user_query = \"\"\"Artificial Intelligence\"\"\""
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AFhbGnWZpWN",
        "outputId": "ddf8984d-4c8b-4ccf-d790-539289cc80fc"
      },
      "source": [
        "# For convenience, I've wrapped all steps in the vector_search function.\n",
        "# It takes four arguments: \n",
        "# A query, the sentence-level transformer, the Faiss index and the number of requested results\n",
        "D, I = vector_search([user_query], model, index, num_results=10)\n",
        "print(f'L2 distance: {D.flatten().tolist()}\\n\\nMAG paper IDs: {I.flatten().tolist()}')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L2 distance: [183.08160400390625, 195.53298950195312, 221.16964721679688, 243.65826416015625, 245.3956756591797, 246.18502807617188, 250.58746337890625, 253.0164794921875, 255.68038940429688, 256.96875]\n",
            "\n",
            "MAG paper IDs: [1164, 2862, 2142, 6596, 5783, 5309, 1456, 6216, 5311, 1349]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbanjBhBZtWZ",
        "outputId": "bf4fa949-2179-4533-da7e-f21dd58c89ce"
      },
      "source": [
        "# Fetching the paper titles based on their index\n",
        "id2details(I)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Artificial intelligence\\n    ',\n",
              " 'AI-complete\\n    ',\n",
              " 'List of artificial intelligence projects\\n    ',\n",
              " 'Computer vision\\n    ',\n",
              " 'Computer program\\n    ',\n",
              " 'Software\\n    ',\n",
              " 'AWK\\n    ',\n",
              " 'Chinese room\\n    ',\n",
              " 'Computer programming\\n    ',\n",
              " 'Atanasoff–Berry computer\\n    ']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbxFKF-DZxg0",
        "outputId": "e85c8c75-c820-41b2-8bcc-2b09b1a501d6"
      },
      "source": [
        "# Define project base directory\n",
        "# Change the index from 1 to 0 if you run this on Google Colab\n",
        "project_dir = Path('notebooks').resolve().parents[0]\n",
        "print(project_dir)\n",
        "\n",
        "# Serialise index and store it as a pickle\n",
        "with open(\"/content/drive/MyDrive/TTDS/faiss_index.pickle\", \"wb\") as h:\n",
        "    pickle.dump(faiss.serialize_index(index), h)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtSC6oDDjtMA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}